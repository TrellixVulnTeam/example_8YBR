{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch. 18 - Temporal Order Matters\n",
    "\n",
    "In language, the order of words matters. The sentences 'The dog lies on the couch' and 'The couch lies on the dog' contain the exact same words yet they describe two very different situations. Our previous model did not take the order of words into account. In this chapter we will take a look at two methods to ensure that your model can access information from the order of words.\n",
    "\n",
    "## 1D Convolutions\n",
    "You might remember convolutional neural networks from computer vision week. In computer vision, convolutional filters slide over the image two dimensionally. There is also a version of convolutional filters that can slide over a sequence one dimensionally. The output is another sequence, much like the output of a two dimensional convolution was another 'image'. Everything else about 1D convolutions is exactly the same as 2D convolutions. \n",
    "\n",
    "To make it a bit easier we can download the IMDB dataset directly through Keras with tokenization already done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_words = 10000  # Our 'vocabulary of 10K words\n",
    "max_len = 500  # Cut texts after 500 words\n",
    "\n",
    "# Get data from Keras\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (25000, 500)\n",
      "x_test shape: (25000, 500)\n"
     ]
    }
   ],
   "source": [
    "# Pad sequences\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the conv model\n",
    "\n",
    "Now we build our convolutional model. You will notice a couple new layers next to ``Conv1D``\n",
    "\n",
    "- [``MaxPooling1D``](https://keras.io/layers/pooling/#maxpooling1d) works exactly like ``MaxPooling2D`` which we used earlier. It takes a piece of the sequence with specified length and returns the maximum element in the sequence much like it returned the maximum element of a small window in 2D convolutional networks. Note that MaxPooling always returns the maximum element for each channel. \n",
    "- [``GlobalMaxPooling2D``](https://keras.io/layers/pooling/#globalmaxpooling1d) returns the maximum over the entire sequence. \n",
    "\n",
    "You can see the difference between the two in the model summary below. While ``MaxPooling1D`` significantly shortens the sequence, ``GlobalMaxPooling2D`` removes the temporal dimension entirely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 494, 32)           22432     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 98, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 92, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,029,665\n",
      "Trainable params: 1,029,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 100, input_length=max_len)) # We train our own embeddings\n",
    "model.add(Conv1D(32, 7, activation='relu')) # 1D Convolution, 32 channels, windows size 7\n",
    "model.add(MaxPooling1D(5)) # Pool windows of size 5\n",
    "model.add(Conv1D(32, 7, activation='relu')) # Another 1D Convolution, 32 channels, windows size 7\n",
    "model.add(GlobalMaxPooling1D()) # Global Pooling\n",
    "model.add(Dense(1)) # Final Output Layer\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 51s - loss: 0.6919 - acc: 0.6169 - val_loss: 0.6068 - val_acc: 0.7440\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 52s - loss: 0.4928 - acc: 0.8061 - val_loss: 0.4585 - val_acc: 0.8196\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=2,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That does not look too bad! 1D Convolutions also have relatively few parameters so they are quick to train.\n",
    "\n",
    "## Reocurrent Neural Networks\n",
    "\n",
    "Another method to make order matter in neural networks is to give the network some kind of memory. So far, all of our networks did a forward pass without any memory of what happened before or after the pass. It is time to change that with reocurrent neural networks.\n",
    "\n",
    "![Simple RNN](./assets/simple_rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reocurrent neural networks contain reocurrent layers. Reocurrent layers can remember their last activation and use it as their own input.\n",
    "\n",
    "$$A_{t} = activation( W * in + U * A_{t-1} + b)$$\n",
    "\n",
    "A reocurrent layer takes a sequence as an input. For each element, it then computes a matrix multiplication ($W * in$) just like a ``Dense`` layer and runs the result through an activation function like e.g. ``relu``. It then retains it's own activation. When the next item of the sequence arrives, it performs the matrix multiplication as before but it also multiplies it's previous activation with a second matrix ($U * A_{t-1}$). It adds the result of both operations together and passes it through it's activation function again. In Keras, we can use a simple rnn like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, None, 100)         1000000   \n",
      "_________________________________________________________________\n",
      "simple_rnn_8 (SimpleRNN)     (None, 32)                4256      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,004,289\n",
      "Trainable params: 1,004,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# No need to specify the sequence length anymore\n",
    "model.add(Embedding(max_words, embedding_dim)) # We train our own embeddings\n",
    "# RNN's only need their size as a parameter, just like Dense layers\n",
    "model.add(SimpleRNN(32, activation='relu'))\n",
    "# Dense output for final classification\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attuned reader might have noticed that we no longer specify an input length in the embeddings layer. That is because RNN's can work with sequences of arbitrary length! If not specified otherwise, a RNN layer will only pass the last output on to the next layer, which is why they have no trouble working with Dense layers. If we want to stack RNN layers, we need to tell them to pass on the entire output sequence so that the following layer has something to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, None, 100)         1000000   \n",
      "_________________________________________________________________\n",
      "simple_rnn_9 (SimpleRNN)     (None, None, 32)          4256      \n",
      "_________________________________________________________________\n",
      "simple_rnn_10 (SimpleRNN)    (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,006,369\n",
      "Trainable params: 1,006,369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# No need to specify the sequence length anymore\n",
    "model.add(Embedding(max_words, embedding_dim)) # We train our own embeddings\n",
    "# This one returns the full sequence\n",
    "model.add(SimpleRNN(32, activation='relu', return_sequences=True))\n",
    "# This one just the last sequence element\n",
    "model.add(SimpleRNN(32, activation='relu'))\n",
    "# Dense output for final classification\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, it is still common to cut sequences after a certain length. Some sequences might just be extremely long and not contain much more valuable information after a certain point. Cutting them off saves on computing power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 54s - loss: 0.5946 - acc: 0.6904 - val_loss: 0.5561 - val_acc: 0.6984\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 53s - loss: 0.4494 - acc: 0.7783 - val_loss: 0.6577 - val_acc: 0.6058\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 53s - loss: 0.4108 - acc: 0.8116 - val_loss: 0.6082 - val_acc: 0.7502\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 53s - loss: 0.5565 - acc: 0.7326 - val_loss: 0.7711 - val_acc: 0.5728\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 54s - loss: 0.5276 - acc: 0.7277 - val_loss: 0.7835 - val_acc: 0.5942\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=5,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see a RNN trains just like other neural nets, although this particular setup performs quite poorly on this task. A problem of RNN's is that their memory is quite short term. While they should in theory be able to tweak their outputs to retain long term memory, they are only really able to retain information about the last one or two words. In the next chapter, we will look at ``LSTM``s that do not have this issue.\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this chapter you have learned about two methods to take the order in sequences into account. 1 dimensional convolution works very similar to convolution as we know it from computer vision. It is also quite fast and uses few parameters. RNN's on the other hand use more parameters but can work with sequences of arbitrary length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
