{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch. 17 - NLP and Word Embeddings\n",
    "\n",
    "Welcome to week 4! This week, we will take a look at natural language processing. From Wikipedia:\n",
    "> Natural language processing (NLP) is a field of computer science, artificial intelligence concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language data.\n",
    "\n",
    "> Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.\n",
    "\n",
    "While last week was about making computers able to see, this week is about making them able to read. This is useful in the financial industry where large amounts of information are usually presented in form of texts. Starting from ticker headlines, to news reports, to analyst reports all the way to off the record chit chat by industry figures on social media, text is in many ways at the very center of what the financial industry does. In this week, we will take a look at text classification problems and sentiment analysis.\n",
    "\n",
    "## Sentiment analysis with the IMDB dataset\n",
    "\n",
    "Sentiment analysis is about judging how positive or negative the tone in a document is. The output of a sentiment analysis is a score between zero and one, where one means the tone is very positive and zero means it is very negative. Sentiment analysis is used for trading quite frequently. For example the sentiment of quarterly reports issued by firms is automatically analyzed to see how the firm judges its own position. Sentiment analysis is also applied to the tweets of traders to estimate an overall market mood. Today, there are many data providers that offer sentiment analysis as a service.\n",
    "\n",
    "In principle, training a sentiment analysis model works just like training a binary text classifier. The text gets classified into positive (1) or not positive (0). This works exactly like other binary classification only that we need some new tools to handle text.\n",
    "\n",
    "A common dataset for sentiment analysis is the corpus of [Internet Movie Database (IMDB)](http://www.imdb.com/) movie reviews. Since each review comes with a text and a numerical rating, the number of stars, it is easy to label the training data. In the IMDB dataset, movie reviews that gave less then five stars where labeled negative while movies that gave more than seven stars where labeled positive (IMDB works with a ten star scale). Let's give the data a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "imdb_dir = './aclImdb' # Data directory\n",
    "train_dir = os.path.join(imdb_dir, 'train') # Get the path of the train set\n",
    "\n",
    "# Setup empty lists to fill\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "# First go through the negatives, then through the positives\n",
    "for label_type in ['neg', 'pos']:\n",
    "    # Get the sub path\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    \n",
    "    # Loop over all files in path\n",
    "    for fname in os.listdir(dir_name):\n",
    "        \n",
    "        # Only consider text files\n",
    "        if fname[-4:] == '.txt':\n",
    "            # Read the text file and put it in the list\n",
    "            f = open(os.path.join(dir_name, fname))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            # Attach the corresponding label\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should have 25,000 texts and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels), len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Half of the reviews are positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a positive review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 1\n",
      "This film is brilliant! It touches everyone who sees it in an extraordinary way. It really takes you back to your youth and puts a new perspective on how you view your childhood memories. There are so many layers to this film. It is innovative and absolutely fabulous!\n"
     ]
    }
   ],
   "source": [
    "print('Label',labels[24002])\n",
    "print(texts[24002])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a negative review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0\n",
      "Well...tremors I, the original started off in 1990 and i found the movie quite enjoyable to watch. however, they proceeded to make tremors II and III. Trust me, those movies started going downhill right after they finished the first one, i mean, ass blasters??? Now, only God himself is capable of answering the question \"why in Gods name would they create another one of these dumpster dives of a movie?\" Tremors IV cannot be considered a bad movie, in fact it cannot be even considered an epitome of a bad movie, for it lives up to more than that. As i attempted to sit though it, i noticed that my eyes started to bleed, and i hoped profusely that the little girl from the ring would crawl through the TV and kill me. did they really think that dressing the people who had stared in the other movies up as though they we're from the wild west would make the movie (with the exact same occurrences) any better? honestly, i would never suggest buying this movie, i mean, there are cheaper ways to find things that burn well.\n"
     ]
    }
   ],
   "source": [
    "print('Label',labels[1])\n",
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing text\n",
    "\n",
    "Computers can not work with words directly. To them, a word is just a meaningless row of characters. To work with words, we need to turn words into so called 'Tokens'. A token is a number that represents that word. Each word gets assigned a token. Tokens are usually assigned by word frequency. The most frequent words like 'a' or 'the' get tokens like 1 or 2 while less often used words like 'profusely' get assigned very high numbers.\n",
    "\n",
    "We can tokenize text directly with Keras. When we tokenize text, we usually choose a maximum number of words we want to consider, our vocabulary so to speak. This prevents us from assigning tokens to words that are hardly ever used, mostly because of typos or because they are not actual words or because they are just very uncommon. This prevents us from over fitting to texts that contain strange words or wired spelling errors. Words that are beyond that cutoff point get assigned the token 0, unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "max_words = 10000 # We will only consider the 10K most used words in this dataset\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words) # Setup\n",
    "tokenizer.fit_on_texts(texts) # Generate tokens by counting frequency\n",
    "sequences = tokenizer.texts_to_sequences(texts) # Turn text into sequence of numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizers word index is a dictionary that maps each word to a number. You can see that words that are frequently used in discussions about movies have a lower token number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token for \"the\" 1\n",
      "Token for \"Movie\" 17\n",
      "Token for \"Movie\" 20126\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Token for \"the\"',word_index['the'])\n",
    "print('Token for \"Movie\"',word_index['movie'])\n",
    "print('Token for \"generator\"',word_index['generator'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our positive review from earlier has now been converted into a sequence of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11,\n",
       " 19,\n",
       " 6,\n",
       " 526,\n",
       " 9,\n",
       " 2437,\n",
       " 313,\n",
       " 34,\n",
       " 1081,\n",
       " 9,\n",
       " 8,\n",
       " 32,\n",
       " 2812,\n",
       " 93,\n",
       " 9,\n",
       " 63,\n",
       " 301,\n",
       " 22,\n",
       " 142,\n",
       " 5,\n",
       " 126,\n",
       " 1934,\n",
       " 2,\n",
       " 1454,\n",
       " 3,\n",
       " 159,\n",
       " 1970,\n",
       " 20,\n",
       " 86,\n",
       " 22,\n",
       " 648,\n",
       " 126,\n",
       " 1546,\n",
       " 1882,\n",
       " 47,\n",
       " 23,\n",
       " 35,\n",
       " 108,\n",
       " 5914,\n",
       " 5,\n",
       " 11,\n",
       " 19,\n",
       " 9,\n",
       " 6,\n",
       " 3964,\n",
       " 2,\n",
       " 424,\n",
       " 2734]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[24002]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To proceed, we now have to make sure that all text sequences we feed into the model have the same length. We can do this with Keras pad sequences tool. It cuts of sequences that are too long and adds zeros to sequences that are too short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 100)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "maxlen = 100 # Make all sequences 100 words long\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "print(data.shape) # We have 25K, 100 word sequences now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can turn all data into proper training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.asarray(labels)\n",
    "\n",
    "# Shuffle data\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "training_samples = 20000  # We will be training on 10K samples\n",
    "validation_samples = 5000  # We will be validating on 10000 samples\n",
    "\n",
    "# Split data\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "As the attuned reader might have already guessed, words and word tokens are categorical features. As such, we can not directly feed them into the neural net. Just because a word has a larger token value, it does not express a higher value in any way. It is just a different category. Previously, we have dealt with categorical data by turning it into one hot encoded vectors. But for words, this is impractical. Since our vocabulary is 10,000 words, each vector would contain 10,000 numbers which are all zeros except for one. This is highly inefficient. Instead we will use an embedding. \n",
    "\n",
    "Embeddings also turn categorical data into vectors. But instead of creating a one hot vector, we create a vector in which all elements are numbers.\n",
    "\n",
    "![Embedding](./assets/embeddings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, embeddings work like a look up table. For each token, they store a vector. When the token is given to the embedding layer, it returns the vector for that token and passes it through the neural network. As the network trains, the embeddings get optimized as well. Remember that neural networks work by calculating the derivative of the loss function with respect to the parameters (weights) of the model. Through backpropagation we can also calculate the derivative of the loss function with respect to the _input_ of the model. Thus we can optimize the embeddings to deliver ideal inputs that help our model. \n",
    "\n",
    "In practice it looks like this: We have to specify how large we want the word vectors to be. A 50 dimensional vector is able to capture good embeddings even for quite large vocabularies. We also have to specify for how many words we want embeddings and how long our sequences are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 50)           500000    \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 5000)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                160032    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 660,065\n",
      "Trainable params: 660,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "embedding_dim = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the embedding layer has 500,000 trainable parameters, that is 50 parameters for each of the 10K words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 3s - loss: 0.5346 - acc: 0.7085 - val_loss: 0.3894 - val_acc: 0.8210\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 3s - loss: 0.1322 - acc: 0.9566 - val_loss: 0.4142 - val_acc: 0.8370\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 3s - loss: 0.0125 - acc: 0.9990 - val_loss: 0.4537 - val_acc: 0.8380\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 3s - loss: 0.0021 - acc: 1.0000 - val_loss: 0.4968 - val_acc: 0.8360\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 3s - loss: 0.0010 - acc: 1.0000 - val_loss: 0.5078 - val_acc: 0.8370\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 3s - loss: 6.1137e-04 - acc: 1.0000 - val_loss: 0.5245 - val_acc: 0.8360\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 3s - loss: 4.0845e-04 - acc: 1.0000 - val_loss: 0.5427 - val_acc: 0.8350\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 3s - loss: 2.8757e-04 - acc: 1.0000 - val_loss: 0.5567 - val_acc: 0.8360\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 3s - loss: 2.1033e-04 - acc: 1.0000 - val_loss: 0.5693 - val_acc: 0.8350\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 3s - loss: 1.5782e-04 - acc: 1.0000 - val_loss: 0.5830 - val_acc: 0.8340\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that training your own embeddings is prone to over fitting. As you can see our model archives 100% accuracy on the training set but only 83% accuracy on the validation set. A clear sign of over fitting. In practice it is therefore quite rare to train new embeddings unless you have a massive dataset. Much more commonly, pre trained embeddings are used. A common pretrained embedding is [GloVe, Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/). It has been trained on billions of words from Wikipedia and the Gigaword 5 dataset, more than we could ever hope to train from our movie reviews. After downloading the GloVe embeddings from the [GloVe website](https://nlp.stanford.edu/projects/glove/) we can load them into our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove_dir = './glove.6B' # This is the folder with the dataset\n",
    "\n",
    "embeddings_index = {} # We create a dictionary of word -> embedding\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt')) # Open file\n",
    "\n",
    "# In the dataset, each line represents a new word embedding\n",
    "# The line starts with the word and the embedding values follow\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0] # The first value is the word, the rest are the values of the embedding\n",
    "    embedding = np.asarray(values[1:], dtype='float32') # Load embedding\n",
    "    embeddings_index[word] = embedding # Add embedding to our embedding dictionary\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all words that are in our IMDB vocabulary might be in the GloVe embeddings though. For missing words it is wise to use random embeddings with the same mean and standard deviation as the GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0044520083, 0.40815714)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a matrix of all embeddings\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean = all_embs.mean() # Calculate mean\n",
    "emb_std = all_embs.std() # Calculate standard deviation\n",
    "emb_mean,emb_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create an embedding matrix holding all word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100 # We now use larger embeddings\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_words, len(word_index)) # How many words are there actually\n",
    "\n",
    "# Create a random matrix with the same mean and std as the embeddings\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embedding_dim))\n",
    "\n",
    "# The vectors need to be in the same position as their index. \n",
    "# Meaning a word with token 1 needs to be in the second row (rows start with zero) and so on\n",
    "\n",
    "# Loop over all words in the word index\n",
    "for word, i in word_index.items():\n",
    "    # If we are above the amount of words we want to use we do nothing\n",
    "    if i >= max_words: \n",
    "        continue\n",
    "    # Get the embedding vector for the word\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    # If there is an embedding vector, put it in the embedding matrix\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This embedding matrix can be used as weights for the embedding layer. This way, the embedding layer uses the pre trained GloVe weights instead of random ones. We can also set the embedding layer to not trainable. This means, Keras won't change the weights of the embeddings while training which makes sense since our embeddings are already trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                320032    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,320,065\n",
      "Trainable params: 320,065\n",
      "Non-trainable params: 1,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen, weights = [embedding_matrix], trainable = False))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we now have far fewer trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 2s - loss: 0.1271 - acc: 0.9584 - val_loss: 1.0561 - val_acc: 0.6800\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 2s - loss: 0.0986 - acc: 0.9730 - val_loss: 1.1435 - val_acc: 0.6840\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 2s - loss: 0.0764 - acc: 0.9836 - val_loss: 1.2219 - val_acc: 0.6760\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 2s - loss: 0.0647 - acc: 0.9874 - val_loss: 1.3341 - val_acc: 0.6920\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 2s - loss: 0.0485 - acc: 0.9932 - val_loss: 1.4150 - val_acc: 0.6860\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 2s - loss: 0.0378 - acc: 0.9969 - val_loss: 1.5264 - val_acc: 0.6740\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 2s - loss: 0.0329 - acc: 0.9970 - val_loss: 1.6081 - val_acc: 0.6720\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 2s - loss: 0.0255 - acc: 0.9984 - val_loss: 1.7429 - val_acc: 0.6710\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 2s - loss: 0.0201 - acc: 0.9993 - val_loss: 1.7580 - val_acc: 0.6750\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 2s - loss: 0.0148 - acc: 0.9995 - val_loss: 1.8321 - val_acc: 0.6720\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our model over fits less but also does worse on the validation set.\n",
    "\n",
    "# Using our model\n",
    "\n",
    "To determine the sentiment of a text, we can now use our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw seq: [[10, 116, 2515, 2515, 23, 1, 115, 33, 23, 1331, 1385, 12, 61, 178, 1, 115, 15, 1706]]\n",
      "padded seq: [[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0   10  116\n",
      "  2515 2515   23    1  115   33   23 1331 1385   12   61  178    1  115\n",
      "    15 1706]]\n",
      "positivity: [[ 0.9042114]]\n"
     ]
    }
   ],
   "source": [
    "# Demo on a positive text\n",
    "my_text = 'I love dogs. Dogs are the best. They are lovely, cuddly animals that only want the best for humans.'\n",
    "\n",
    "seq = tokenizer.texts_to_sequences([my_text])\n",
    "print('raw seq:',seq)\n",
    "seq = pad_sequences(seq, maxlen=maxlen)\n",
    "print('padded seq:',seq)\n",
    "prediction = model.predict(seq)\n",
    "print('positivity:',prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw seq: [[1, 3763, 7013, 77, 1144, 108, 389, 80]]\n",
      "padded seq: [[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    1 3763 7013   77 1144  108\n",
      "   389   80]]\n",
      "positivity: [[ 0.23451628]]\n"
     ]
    }
   ],
   "source": [
    "# Demo on a negative text\n",
    "my_text = 'The bleak economic outlook will force many small businesses into bankruptcy.'\n",
    "\n",
    "seq = tokenizer.texts_to_sequences([my_text])\n",
    "print('raw seq:',seq)\n",
    "seq = pad_sequences(seq, maxlen=maxlen)\n",
    "print('padded seq:',seq)\n",
    "prediction = model.predict(seq)\n",
    "print('positivity:',prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings as semantic geometry\n",
    "\n",
    "One very interesting aspect of embeddings trained on large numbers of words is that they show patterns in which the geometric relationship between word vectors corresponds to the semantic relationship between these words.\n",
    "\n",
    "![Relations](./assets/man_woman.jpg)\n",
    "\n",
    "\n",
    "In the picture above for instance you can see that the direction of feminine words to their male counterparts is roughly the same. In other words, if you where to substract the word vector for 'woman' from the word 'queen' and add the word vector for 'man' you would arrive at 'king'. This also works for other relationships like comparatives and superlatives. \n",
    "\n",
    "![Rel Comp Sup](./assets/comparative_superlative.jpg)\n",
    "\n",
    "This highlights some interesting properties of language in which semantic meanings can be seen as directions which can be added or subtracted.\n",
    "\n",
    "A sad side effect of training word vectors on human writing is that it captures human biases. For example it has been [shown](https://www.technologyreview.com/s/602025/how-vector-space-mathematics-reveals-the-hidden-sexism-in-language/) that for word vectors trained on news websites, 'Programmer' - 'Man' + 'Woman' equals 'Homemaker' reflecting the bias in language that assigns the role of homemaker more often to woman than men. Measuring these biases in embeddings and correcting them has become a field of [research on its own](https://www.technologyreview.com/s/602025/how-vector-space-mathematics-reveals-the-hidden-sexism-in-language/) which highlights how even professional writing from news outlets can be biased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this chapter you have taken the first steps into natural language processing. You have learned about tokenization and word embeddings. You have learned how to train your own embeddings and how to load pre trained embeddings into your model."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
