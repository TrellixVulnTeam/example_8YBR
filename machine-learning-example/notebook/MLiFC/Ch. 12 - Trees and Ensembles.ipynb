{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch. 12 - Trees and Ensembles\n",
    "So far, we have covered neural networks only. However, there are many, many more. The reason we have focused on neural nets so far (and will continue to do so throughout the material) is because neural nets work well on many different tasks. From classifying images to generating music, neural nets can accomplish many tasks. For structured data however, decision trees still make a strong showing. So in this chapter, we will have a look at them together with the powerful concept of ensembles: the combination of many models into one.\n",
    "\n",
    "As always, before we start, let's load some basic libraries and the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Set seed for reproducability \n",
    "np.random.seed(42)\n",
    "import matplotlib.pyplot as plt\n",
    "# Supress warnings for better readability\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('processed_bank.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y</th>\n",
       "      <th>job_admin.</th>\n",
       "      <th>...</th>\n",
       "      <th>month_sep</th>\n",
       "      <th>day_of_week_fri</th>\n",
       "      <th>day_of_week_mon</th>\n",
       "      <th>day_of_week_thu</th>\n",
       "      <th>day_of_week_tue</th>\n",
       "      <th>day_of_week_wed</th>\n",
       "      <th>poutcome_failure</th>\n",
       "      <th>poutcome_nonexistent</th>\n",
       "      <th>poutcome_success</th>\n",
       "      <th>contacted_before</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34579</th>\n",
       "      <td>-0.68711</td>\n",
       "      <td>-2.555677e-13</td>\n",
       "      <td>0.976408</td>\n",
       "      <td>-0.758550</td>\n",
       "      <td>-0.928102</td>\n",
       "      <td>-1.122929</td>\n",
       "      <td>-0.900202</td>\n",
       "      <td>-0.418322</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>-0.68711</td>\n",
       "      <td>-2.555677e-13</td>\n",
       "      <td>-0.452557</td>\n",
       "      <td>0.924213</td>\n",
       "      <td>0.806766</td>\n",
       "      <td>0.705671</td>\n",
       "      <td>0.998971</td>\n",
       "      <td>0.637509</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20173</th>\n",
       "      <td>-0.13552</td>\n",
       "      <td>-2.555677e-13</td>\n",
       "      <td>-0.452557</td>\n",
       "      <td>1.098292</td>\n",
       "      <td>-0.059880</td>\n",
       "      <td>0.761649</td>\n",
       "      <td>1.056089</td>\n",
       "      <td>1.063747</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18171</th>\n",
       "      <td>-0.13552</td>\n",
       "      <td>-2.555677e-13</td>\n",
       "      <td>-0.452557</td>\n",
       "      <td>1.098292</td>\n",
       "      <td>0.687012</td>\n",
       "      <td>-0.469858</td>\n",
       "      <td>1.055031</td>\n",
       "      <td>1.063747</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30128</th>\n",
       "      <td>-0.68711</td>\n",
       "      <td>-2.555677e-13</td>\n",
       "      <td>-0.452557</td>\n",
       "      <td>-0.758550</td>\n",
       "      <td>-0.641321</td>\n",
       "      <td>-1.290862</td>\n",
       "      <td>-0.847844</td>\n",
       "      <td>-0.418322</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       campaign         pdays  previous  emp.var.rate  cons.price.idx  \\\n",
       "34579  -0.68711 -2.555677e-13  0.976408     -0.758550       -0.928102   \n",
       "446    -0.68711 -2.555677e-13 -0.452557      0.924213        0.806766   \n",
       "20173  -0.13552 -2.555677e-13 -0.452557      1.098292       -0.059880   \n",
       "18171  -0.13552 -2.555677e-13 -0.452557      1.098292        0.687012   \n",
       "30128  -0.68711 -2.555677e-13 -0.452557     -0.758550       -0.641321   \n",
       "\n",
       "       cons.conf.idx  euribor3m  nr.employed  y  job_admin.        ...         \\\n",
       "34579      -1.122929  -0.900202    -0.418322  0           1        ...          \n",
       "446         0.705671   0.998971     0.637509  1           0        ...          \n",
       "20173       0.761649   1.056089     1.063747  1           1        ...          \n",
       "18171      -0.469858   1.055031     1.063747  1           1        ...          \n",
       "30128      -1.290862  -0.847844    -0.418322  0           0        ...          \n",
       "\n",
       "       month_sep  day_of_week_fri  day_of_week_mon  day_of_week_thu  \\\n",
       "34579          0                0                0                1   \n",
       "446            0                0                0                0   \n",
       "20173          0                0                1                0   \n",
       "18171          0                0                0                0   \n",
       "30128          0                0                0                1   \n",
       "\n",
       "       day_of_week_tue  day_of_week_wed  poutcome_failure  \\\n",
       "34579                0                0                 1   \n",
       "446                  1                0                 0   \n",
       "20173                0                0                 0   \n",
       "18171                0                1                 0   \n",
       "30128                0                0                 0   \n",
       "\n",
       "       poutcome_nonexistent  poutcome_success  contacted_before  \n",
       "34579                     0                 0                 0  \n",
       "446                       1                 0                 0  \n",
       "20173                     1                 0                 0  \n",
       "18171                     1                 0                 0  \n",
       "30128                     1                 0                 0  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that data is okay\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data into train / dev / test\n",
    "# X is everything that is not y\n",
    "X = df.loc[:, df.columns != 'y'].values\n",
    "# y is y\n",
    "y = df['y'].values\n",
    "\n",
    "# First split in train / test_dev\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test_dev, y_train, y_test_dev = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Second split in dev / test\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X_test_dev, y_test_dev, test_size=0.5, random_state=0)\n",
    "\n",
    "# Remove test_dev set from memory\n",
    "del X_test_dev\n",
    "del y_test_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree\n",
    "The best way to understand a decision tree is to look at one. This tree is what we will create a few lines from here:\n",
    "![decision tree](./assets/Decision_tree.png)\n",
    "\n",
    "As you can see, a decision tree starts out by splitting the dataset into two datasets by one variable, in this case the employment. The goal is to arrive at sets that are either uniformly part of the yes crowd or uniformly nay sayers. So in this case, the algorithm has determined that a certain threshold in employment is the best separator of the two classes. It then recursively proceeds until it has found perfectly uniform groups or gets stopped by a maximum depth parameter. A little bit more formally, a tree classifier tries to minimize [gini impurity](https://datascience.stackexchange.com/questions/10228/gini-impurity-vs-entropy) across subsets. Gini impurity is the probability that a randomly chosen item from a set would be incorrectly labeled by the label of another random item from the same set. On a perfectly uniform set, this impurity would be zero.\n",
    "\n",
    "The big advantage of decision trees is that they are easy to interpret. You can just look at the tree and see which variables matter. The disadvantage is that they are somewhat weak classifiers. They struggle to learn complex functions without over fitting. \n",
    "\n",
    "Decision trees can be trained with [sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the corresponding class\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prevent over fitting we usually need to set a maximum depth we want to allow. A common choice is 6 layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_classifier = DecisionTreeClassifier(max_depth=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As all sklearn classes, ``DecisionTreeClassifier`` can be trained with ``.fit()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_classifier.fit(X=X_train,y=y_train); # ; suppresses the output of the cell for cleaner reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test our tree on the dev set with the ``.score()``function, which outputs the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72931034482758617"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_classifier.score(X_dev,y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "72.9% accuracy is not bad for a simple tree, especially one that was much faster to train than a neural net. However, it lags behind. This is where a truly powerful idea comes into play. What if we combined many weak classifiers into a strong one?\n",
    "\n",
    "## Bagging\n",
    "Bagging (short for bootstrap aggregating) takes the idea further. It does not only train multiple classifiers but also trains them on different subsets of the data. This means that even if each classifier over fits 'their' subset, in aggregate they will not over fit the dataset. Sklearn also has an [implementation for Bagging](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import class\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup, needs the classifier we want to use and the number of classifiers\n",
    "bagger = BaggingClassifier(base_estimator=tree_classifier, n_estimators=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit will train the specified number of classifiers\n",
    "bagger.fit(X_train,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73448275862068968"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagger.score(X_dev,y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, many models do better than one. We can take this concept even further if we use some more tricks to avoid that the decision trees we train become too similar.\n",
    "\n",
    "## Random forests \n",
    "Remember how a decision tree recursively splits the dataset by one variable that leads to more uniformity. In random forests, we give the splitting algorithm only access to a random subset of features at each step. This will lead to a bigger variety of trees as each branch of the trees has to work with different features. Therefore, we can avoid over fitting even more, but gain performance by training more trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomforest = RandomForestClassifier(max_depth=6,n_estimators=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomforest.fit(X_train,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73965517241379308"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomforest.score(X_dev,y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest outperforms the decision tree and the simple bagging algorithm. It also performs slightly better than the best neural net from Ch. 10. Their good performance on structured data and their ease of training make random forests still a popular choice for working with structured data. \n",
    "\n",
    "## Gradient boosting\n",
    "Another very popular technique for structural data is gradient boosting. While gradient boosting theoretically works with any machine learning algorithm (neural networks, too), it is mostly done with decision trees in practice. Gradient boosting works by iteratively adding classifiers that aim to reduce the residuals of the previous model.\n",
    "Say we have a classifier $F_m(x)$. We then fit a residuals classifier $h(x)$:\n",
    "\n",
    "$$h(x) = y - F_m(x)$$\n",
    "\n",
    "We then add the residuals classifier to our original classifier to obtain a better model:\n",
    "\n",
    "$$F_{m+1}(x)=F_m(x)+h(x)=y$$\n",
    "\n",
    "We can do this over and over, obtaining a better and better classifier. Take a look at how we compute $h(x)$ for a minute. It looks like the the gradient of the loss function from our neural networks in week 1! And in fact it is the gradient of the loss of the gradient boosting classifier. Gradient boosting does a form of gradient descent, too! There even is a learning rate, which is called shrinkage in gradient boosting.\n",
    "\n",
    "$$F_{m+1}(x)=F_m(x)+\\alpha * h(x)=y$$\n",
    "\n",
    "The most popular implementation of gradient boosting is [XGBoost](https://github.com/dmlc/xgboost). XGBoost uses decision trees as a base model and is designed to be highly scalable. It also features many more tricks to make gradient boosting work better. You don't have to know all the frills, the standard parameters usually work quite well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jannes/anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Get XGBoost and import the classifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameteris:\n",
    "# Learning rate = alpha\n",
    "# max_depth maximum depth of each tree\n",
    "xgclassifier = XGBClassifier(learning_rate=0.1,max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifier\n",
    "xgclassifier.fit(X_train,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74224137931034484"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scoring works exactly as with sklearn\n",
    "xgclassifier.score(X_dev,y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost achieves an accuracy of over 74.2%, that is better than the best neural network we came up with. It also trained a lot faster than a neural net. This is why it is still very popular for structured data. However, it reaches its limits on unstructured data like text, images or sound.\n",
    "\n",
    "## Stacking\n",
    "Stacking is another ensemble technique. In stacking, we train a classifier using the outputs of other classifiers as input features. This image shows how it is done to [win kaggle competitions](http://blog.kaggle.com/2016/08/31/avito-duplicate-ads-detection-winners-interview-2nd-place-team-the-quants-mikel-peter-marios-sonny/):\n",
    "![stacking](./assets/the_quants_ensemble_stacking.png)\n",
    "\n",
    "Let's see how we could use stacking with the classifiers we have developed so far in this chapter. Sadly there is no easy sklearn method we could use for stacking so we will build our own. It is helpful to train the base and meta classifiers on different datasets to avoid over fitting, so first we will split our train data into a base training and a meta training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train set into meta and base training sets\n",
    "X_base, X_meta, y_base, y_meta = train_test_split(X_train, y_train, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then fit all of our base classifiers to the base training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_classifier.fit(X_base,y_base);\n",
    "bagger.fit(X_base,y_base);\n",
    "randomforest.fit(X_base,y_base);\n",
    "xgclassifier.fit(X_base,y_base);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For good measure we can also throw in our neural net from chapter 11:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net = load_model('./support_files/Ch11_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create an input data set for the meta classifier by letting our models make predictions on the meta training set. Note that we must reshape all of our predictions to make sure they have the same shape when we feed them into the meta classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction from sigle tree classifier\n",
    "treepred = tree_classifier.predict(X_meta).reshape(X_meta.shape[0],1)\n",
    "# Get prediction from bagged tree classifier\n",
    "baggerpred = bagger.predict(X_meta).reshape(X_meta.shape[0],1)\n",
    "# Get prediction from random forrest\n",
    "forestpred = randomforest.predict(X_meta).reshape(X_meta.shape[0],1)\n",
    "# Get prediction from XGBoost\n",
    "xgpred = xgclassifier.predict(X_meta).reshape(X_meta.shape[0],1)\n",
    "# Get prediction from neural net\n",
    "nnpred = neural_net.predict(X_meta).reshape(X_meta.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine predictions into meta features\n",
    "meta_features = np.stack((treepred,baggerpred,forestpred,xgpred,nnpred),axis=1).reshape(X_meta.shape[0],5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then train a meta classifier, let's make it another xgboost here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the meta classifier\n",
    "meta = XGBClassifier()\n",
    "meta.fit(meta_features,y_meta);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions we will define a new method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(X):\n",
    "    # Get meta predictions\n",
    "    treepred = tree_classifier.predict(X).reshape(X.shape[0],1)\n",
    "    baggerpred = bagger.predict(X).reshape(X.shape[0],1)\n",
    "    forestpred = randomforest.predict(X).reshape(X.shape[0],1)\n",
    "    xgpred = xgclassifier.predict(X).reshape(X.shape[0],1)\n",
    "    nnpred = neural_net.predict(X).reshape(X.shape[0],1)\n",
    "    # Combine predictions\n",
    "    meta_features = np.stack((treepred,baggerpred,forestpred,xgpred,nnpred),axis=1).reshape(X.shape[0],5)\n",
    "    # Make meta predictions\n",
    "    meta_pred = meta.predict(meta_features)\n",
    "    return meta_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lack of a ``.score()`` method we can measure the accuracy with sklearns accuracy score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = make_predictions(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73103448275862071"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn predictions into definit predictions \n",
    "predictions[predictions >= 0.5] = 1\n",
    "predictions[predictions < 0.5] = 0\n",
    "\n",
    "# Measure accuracy\n",
    "accuracy_score(y_dev,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our stacked model does worse than our best single model (XGBoost). This does happen. But in general, stacking can help make the model more robust and increase accuracy. There are many more interesting aspects to stacking that we will not cover in this material but that you might want to think about: What happens for example, if you train your base models on a specific subset of the data. Say, you create a base model that is only trained on young people from your dataset. How would you create a meta model that takes this into account? Or how many models do you think are practical? We could generate thousands of models but would it be worth the computational cost? What kind of model could capture the complexity of our overall function as well as the ensemble can, and how could we prevent it from over fitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this chapter, you have seen some popular ensemble techniques and learned about decision trees. These tools are quite useful and are sort of the standard recipe together with neural nets for winning machine learning competitions. Trying out a few of these might well help you in this weeks competition. Good luck!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
